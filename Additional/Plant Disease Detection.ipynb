{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a93fca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.6.2-py3-none-win_amd64.whl (125.4 MB)\n",
      "     ------------------------------------ 125.4/125.4 MB 266.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from xgboost) (1.19.5)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.9.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71ffea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting unzip\n",
      "  Downloading unzip-1.0.0.tar.gz (704 bytes)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: unzip\n",
      "  Building wheel for unzip (setup.py): started\n",
      "  Building wheel for unzip (setup.py): finished with status 'done'\n",
      "  Created wheel for unzip: filename=unzip-1.0.0-py3-none-any.whl size=1279 sha256=9a3afd2be192ffe8ce72de0dc10991c5635be64cfbfe6cf09b780c6b0a5702f6\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\8f\\99\\16\\edb5783f50c3ef92bf49f215ec6f48372dba0adc85fa4d2869\n",
      "Successfully built unzip\n",
      "Installing collected packages: unzip\n",
      "Successfully installed unzip-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd4673f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access denied with the following error:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " \tCannot retrieve the public link of the file. You may need to change\n",
      "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\t https://drive.google.com/uc?id=18DbC6Xj4NP-hLzI14WuMaAEyq482vNfn \n",
      "\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# # Download a file based on its file ID.\n",
    "file_id = '18DbC6Xj4NP-hLzI14WuMaAEyq482vNfn'\n",
    "\n",
    "# # Download dataset\n",
    "!gdown https://drive.google.com/uc?id={file_id}\n",
    "\n",
    "# # Unzip the downloaded file\n",
    "!unzip -q PlantVillage.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea69b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D,Activation,LeakyReLU,BatchNormalization,MaxPooling2D,Flatten,Dense,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d5a986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelBinarizer,MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c51b0ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pepper__bell___Bacterial_spot',\n",
       " 'Pepper__bell___healthy',\n",
       " 'Potato___Early_blight',\n",
       " 'Potato___healthy',\n",
       " 'Potato___Late_blight',\n",
       " 'Tomato_Bacterial_spot',\n",
       " 'Tomato_Early_blight',\n",
       " 'Tomato_healthy',\n",
       " 'Tomato_Late_blight',\n",
       " 'Tomato_Leaf_Mold',\n",
       " 'Tomato_Septoria_leaf_spot',\n",
       " 'Tomato_Spider_mites_Two_spotted_spider_mite',\n",
       " 'Tomato__Target_Spot',\n",
       " 'Tomato__Tomato_mosaic_virus',\n",
       " 'Tomato__Tomato_YellowLeaf__Curl_Virus',\n",
       " 'train']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = r'C:/Users/HP/Downloads/Sir adeel dataset/archive/PlantVillage/'\n",
    "\n",
    "os.chdir(root_dir)\n",
    "listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e47f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of resized image\n",
    "DEFAULT_IMAGE_SIZE = tuple((256, 256))\n",
    "\n",
    "# Number of images used to train the model\n",
    "N_IMAGES = 100\n",
    "\n",
    "data_dir = os.path.join(root_dir,'Pepper__bell___Bacterial_spot')\n",
    "\n",
    "\"\"\"We use the function `convert_image_to_array` to resize an image to the size `DEFAULT_IMAGE_SIZE` we defined above.\"\"\"\n",
    "\n",
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv2.imread(image_dir)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, DEFAULT_IMAGE_SIZE)   \n",
    "            return img_to_array(image)\n",
    "        else:\n",
    "            return np.array([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "27d54a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load images from all classes ...\n",
      "997\n"
     ]
    }
   ],
   "source": [
    "print(\"Load images from all classes ...\")\n",
    "plant_disease_folder_list = listdir(data_dir)\n",
    "print(len(plant_disease_folder_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f146bf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loding Image Data ...\n",
      "Processing  0022d6b7-d47c-4ee2-ae9a-392a53f48647___JR_B.Spot 8964.JPG ...\n",
      "Error : [WinError 267] The directory name is invalid: 'C:/Users/HP/Downloads/Sir adeel dataset/archive/PlantVillage/Pepper__bell___Bacterial_spot/0022d6b7-d47c-4ee2-ae9a-392a53f48647___JR_B.Spot 8964.JPG'\n"
     ]
    }
   ],
   "source": [
    "image_list, label_list = [], []\n",
    "\n",
    "try:\n",
    "    print(\"Loding Image Data ...\")\n",
    "    for s in listdir():\n",
    "        plant_disease_folder_list = listdir(s)\n",
    "\n",
    "        for plant_disease_folder in plant_disease_folder_list:\n",
    "            print(f\"Processing  {plant_disease_folder} ...\")\n",
    "            plant_disease_image_list = listdir(f\"{data_dir}/{plant_disease_folder}\")\n",
    "\n",
    "            for image in plant_disease_image_list[:N_IMAGES]:\n",
    "                image_directory = f\"{data_dir}/{plant_disease_folder}/{image}\"\n",
    "                if image_directory.endswith(\".jpg\")==True or image_directory.endswith(\".JPG\")==True:\n",
    "                    image_list.append(convert_image_to_array(image_directory))\n",
    "                    label_list.append(plant_disease_folder)\n",
    "\n",
    "    print(\"All the images have successfully loaded!!\")  \n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac4cc818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f42f63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 0\n"
     ]
    }
   ],
   "source": [
    "# Transform the loaded training image data into numpy array\n",
    "np_image_list = np.array(image_list, dtype=np.float16) / 255.0\n",
    "\n",
    "# Check the number of images loaded for training\n",
    "image_len = len(image_list)\n",
    "print(f\"Total number of images: {image_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e259fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Examine the labels/classes in the training dataset.\"\"\"\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "image_labels = label_binarizer.fit_transform(label_list)\n",
    "\n",
    "# pickle.dump(label_binarizer,open('plant_disease_label_transform.pkl', 'wb'))\n",
    "n_classes = len(label_binarizer.classes_)\n",
    "\n",
    "print(\"Total number of classes: \", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f5b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Augment and Split Dataset\n",
    "Using `ImageDataGenerator` to augment data by performing various operations on the training images.\n",
    "\"\"\"\n",
    "\n",
    "augment = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "                             height_shift_range=0.1, shear_range=0.2, \n",
    "                             zoom_range=0.2, horizontal_flip=True, \n",
    "                             fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403af4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splitting the data into training and test sets for validation purpose.\"\"\"\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size=0.2, random_state = 42)\n",
    "print('Successfully split data into TRAIN & TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f2b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Build Model\n",
    "Defining the hyperparameters of the plant disease classification model.\n",
    "\"\"\"\n",
    "\n",
    "EPOCHS = 10\n",
    "STEPS = 100\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "DEPTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bd2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating a Sequential Model to build CNN for multi-class classification\"\"\"\n",
    "\n",
    "model = Sequential()\n",
    "inputShape = (HEIGHT, WIDTH, DEPTH)\n",
    "chanDim = -1\n",
    "\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "    inputShape = (DEPTH, HEIGHT, WIDTH)\n",
    "    chanDim = 1\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024, name = 'my_dense'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e590ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'my_dense'\n",
    "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output) \n",
    "intermediate_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb435b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Train Model\n",
    "We initialize Adam optimizer with learning rate and decay parameters. \n",
    "Also, we choose the type of loss and metrics for the model and compile it for training.\n",
    "\"\"\"\n",
    "\n",
    "# steps_per_epoch=len(x_train) // BATCH_SIZE,\n",
    "\n",
    "# Initialize optimizer\n",
    "opt = Adam(learning_rate=LR, decay=LR / EPOCHS)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model\n",
    "print(\"Training CNN...\")\n",
    "history = model.fit(augment.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
    "                              validation_data=(x_test, y_test),\n",
    "                              epochs=20, \n",
    "                              verbose=1)\n",
    "        \n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Evaluate Model\n",
    "Comparing the accuracy and loss by plotting the graph for training and validation.\n",
    "\"\"\"\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Train and validation accuracy\n",
    "plt.plot(epochs, acc, 'b', label='Training accurarcy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\n",
    "plt.title('Training and Validation accurarcy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Train and validation loss\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Evaluating model accuracy by using the `evaluate` method\"\"\"\n",
    "\n",
    "print(\"[INFO] Calculating model accuracy\")\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the Model\n",
    "\n",
    "# model.save('PDD_completemodel')\n",
    "# model.save('PDD_completemodel.h5')\n",
    "# intermediate_layer_model.save('PDD_IntermediateModel')\n",
    "# intermediate_layer_model.save('PDD_IntermediateModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcd58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_predict = intermediate_layer_model.predict(x_train)\n",
    "# print(x_train_predict.shape)\n",
    "\n",
    "x_test_predict = intermediate_layer_model.predict(x_test)\n",
    "print(x_test_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77062d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf')\n",
    "\n",
    "svm.fit(x_train_predict,np.argmax(y_train,axis=1))\n",
    "\n",
    "print('SVM Fit Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.score(x_train_predict,np.argmax(y_train,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42874a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.score(x_test_predict,np.argmax(y_test,axis=1))\n",
    "\n",
    "#Save the SVM model in pickle file\n",
    "# pickle.dump(svm,open('svms.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be6cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred_labels = svm.predict(x_test_predict)\n",
    "Pred_labels = pd.DataFrame(Pred_labels,index =None)\n",
    "Pred_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xb = xgb.XGBClassifier(use_label_encoder=False)\n",
    "\n",
    "xb.fit(x_train_predict,np.argmax(y_train,axis=1))\n",
    "\n",
    "print('XGBoost Fit Complete')\n",
    "\n",
    "#Save XGBoost Model\n",
    "# pickle.dump(xb,open('xgb_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11baea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb.score(x_train_predict,np.argmax(y_train,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb.score(x_test_predict,np.argmax(y_test,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702813f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# ai = load_model('D:\\PDD_Models\\HDF5\\PDD_Intermediatemodel.h5')\n",
    "# xgb_model = xgb.XGBClassifier()\n",
    "# xgb_model.load_model(r'D:\\PDD_Models\\xgb_model.json')\n",
    "\n",
    "# x2 = ai.predict(x_test)\n",
    "# xgb_model.score(x_test_predict,np.argmax(y_test,axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
